{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiRNN-CRF vs Simple CRF Evaluation\n",
    "\n",
    "This notebook compares performance between BiRNN-CRF and simple stationary CRF models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /Users/elenaporras/anaconda3/envs/nlp-class/lib/python3.9/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/elenaporras/anaconda3/envs/nlp-class/lib/python3.9/site-packages (from seaborn) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/elenaporras/anaconda3/envs/nlp-class/lib/python3.9/site-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/elenaporras/anaconda3/envs/nlp-class/lib/python3.9/site-packages (from seaborn) (3.5.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/elenaporras/anaconda3/envs/nlp-class/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/elenaporras/anaconda3/envs/nlp-class/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/elenaporras/anaconda3/envs/nlp-class/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/elenaporras/anaconda3/envs/nlp-class/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/elenaporras/anaconda3/envs/nlp-class/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/elenaporras/anaconda3/envs/nlp-class/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/elenaporras/anaconda3/envs/nlp-class/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/elenaporras/anaconda3/envs/nlp-class/lib/python3.9/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/elenaporras/anaconda3/envs/nlp-class/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install seaborn\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "from pathlib import Path\n",
    "import time, os\n",
    "\n",
    "from corpus import TaggedCorpus\n",
    "from lexicon import build_lexicon\n",
    "from crf_backprop import ConditionalRandomFieldBackprop\n",
    "from crf_neural import ConditionalRandomFieldNeural\n",
    "from eval import model_cross_entropy, viterbi_error_rate\n",
    "\n",
    "os.chdir(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modify train_and_evaluate function\n",
    "def train_and_evaluate(model_class, train_corpus, eval_corpus, **kwargs) -> Dict:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create model using training corpus vocab/tagset\n",
    "    model = model_class(train_corpus.tagset, train_corpus.vocab, **kwargs)\n",
    "    \n",
    "    # Ensure eval corpus uses same vocab/tagset \n",
    "    if not hasattr(eval_corpus, 'tagset'):\n",
    "        eval_corpus = TaggedCorpus('endev', tagset=train_corpus.tagset, vocab=train_corpus.vocab)\n",
    "    \n",
    "    loss = lambda x: model_cross_entropy(x, eval_corpus)\n",
    "\n",
    "    # Rest of function remains the same\n",
    "    model.train(\n",
    "        corpus=train_corpus,\n",
    "        loss=loss,\n",
    "        minibatch_size=kwargs.get('batch_size', 30),\n",
    "        lr=kwargs.get('lr', 0.05), \n",
    "        reg=kwargs.get('reg', 0.0),\n",
    "        max_steps=kwargs.get('max_steps', 2000)\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'dev_cross_entropy': model_cross_entropy(model, eval_corpus),\n",
    "        'dev_error_rate': viterbi_error_rate(model, eval_corpus),\n",
    "        'train_cross_entropy': model_cross_entropy(model, train_corpus),\n",
    "        'train_error_rate': viterbi_error_rate(model, train_corpus),\n",
    "        'training_time': training_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# Load data \n",
    "train_corpus = TaggedCorpus(Path('ensup'))  \n",
    "eval_corpus = TaggedCorpus(Path('endev'), tagset=train_corpus.tagset, vocab=train_corpus.vocab)\n",
    "\n",
    "\n",
    "# Create lexicon\n",
    "lexicon = build_lexicon(train_corpus, embeddings_file=Path('words-10.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BiRNN-CRF vs Simple CRF Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The corpus that this sentence came from uses a different tagset or vocab",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Baseline CRF\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m baseline_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mConditionalRandomFieldBackprop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_corpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdev_corpus\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# BiRNN-CRF\u001b[39;00m\n\u001b[1;32m      9\u001b[0m birnn_results \u001b[38;5;241m=\u001b[39m train_and_evaluate(\n\u001b[1;32m     10\u001b[0m     ConditionalRandomFieldNeural,\n\u001b[1;32m     11\u001b[0m     train_corpus,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     corpus\u001b[38;5;241m=\u001b[39mtrain_corpus\n\u001b[1;32m     16\u001b[0m )\n",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model_class, train_corpus, eval_corpus, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(train_corpus\u001b[38;5;241m.\u001b[39mtagset, train_corpus\u001b[38;5;241m.\u001b[39mvocab, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: model_cross_entropy(x, eval_corpus)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_corpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminibatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_steps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n",
      "File \u001b[0;32m~/jhu/senior_year/nlp/nlp_HW7/nlp_hw7/hw-rnn/code/crf_backprop.py:149\u001b[0m, in \u001b[0;36mConditionalRandomFieldBackprop.train\u001b[0;34m(self, corpus, minibatch_size, lr, reg, tolerance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Now just call the parent method with exactly the same arguments.  The parent\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# method will train exactly as before (you should probably review it) --\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# except that it will call methods that you will override below. Your\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# overridden versions can use the optimizer that we just created.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()   \u001b[38;5;66;03m# this line is here just in case you're using an old version of parent class that doesn't do it\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminibatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminibatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jhu/senior_year/nlp/nlp_HW7/nlp_hw7/hw-rnn/code/crf.py:281\u001b[0m, in \u001b[0;36mConditionalRandomField.train\u001b[0;34m(self, corpus, loss, tolerance, minibatch_size, eval_interval, lr, reg, max_steps, save_path)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad()     \u001b[38;5;66;03m# get ready to accumulate their gradient\u001b[39;00m\n\u001b[1;32m    280\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 281\u001b[0m old_loss \u001b[38;5;241m=\u001b[39m \u001b[43m_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# evaluate initial loss\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m evalbatch \u001b[38;5;129;01min\u001b[39;00m more_itertools\u001b[38;5;241m.\u001b[39mbatched(\n\u001b[1;32m    283\u001b[0m                    itertools\u001b[38;5;241m.\u001b[39mislice(corpus\u001b[38;5;241m.\u001b[39mdraw_sentences_forever(), \n\u001b[1;32m    284\u001b[0m                                     max_steps),  \u001b[38;5;66;03m# limit infinite iterator\u001b[39;00m\n\u001b[1;32m    285\u001b[0m                    eval_interval): \u001b[38;5;66;03m# group into \"evaluation batches\"\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m tqdm(evalbatch, total\u001b[38;5;241m=\u001b[39meval_interval):\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;66;03m# Accumulate the gradient of log p(tags | words) on this sentence \u001b[39;00m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;66;03m# into A_counts and B_counts.\u001b[39;00m\n",
      "File \u001b[0;32m~/jhu/senior_year/nlp/nlp_HW7/nlp_hw7/hw-rnn/code/crf.py:255\u001b[0m, in \u001b[0;36mConditionalRandomField.train.<locals>._loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_loss\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# Evaluate the loss on the current parameters.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m# This will print its own log messages.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# gradient and we can save time by turning off the extra bookkeeping\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# needed to compute it.\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# type: ignore \u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m, in \u001b[0;36mtrain_and_evaluate.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(train_corpus\u001b[38;5;241m.\u001b[39mtagset, train_corpus\u001b[38;5;241m.\u001b[39mvocab, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m----> 6\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mmodel_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_corpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m      9\u001b[0m     corpus\u001b[38;5;241m=\u001b[39mtrain_corpus,\n\u001b[1;32m     10\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     max_steps\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_steps\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/jhu/senior_year/nlp/nlp_HW7/nlp_hw7/hw-rnn/code/eval.py:33\u001b[0m, in \u001b[0;36mmodel_cross_entropy\u001b[0;34m(model, eval_corpus)\u001b[0m\n\u001b[1;32m     31\u001b[0m token_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gold \u001b[38;5;129;01min\u001b[39;00m tqdm(eval_corpus, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(eval_corpus)):\n\u001b[0;32m---> 33\u001b[0m     logprob \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogprob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_corpus\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     34\u001b[0m     token_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(gold) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m    \u001b[38;5;66;03m# count EOS but not BOS\u001b[39;00m\n\u001b[1;32m     35\u001b[0m cross_entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlogprob \u001b[38;5;241m/\u001b[39m token_count\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-class/lib/python3.9/site-packages/typeguard/__init__.py:1033\u001b[0m, in \u001b[0;36mtypechecked.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m memo \u001b[38;5;241m=\u001b[39m _CallMemo(python_func, _localns, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m   1032\u001b[0m check_argument_types(memo)\n\u001b[0;32m-> 1033\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     check_return_type(retval, memo)\n",
      "File \u001b[0;32m~/jhu/senior_year/nlp/nlp_HW7/nlp_hw7/hw-rnn/code/crf.py:339\u001b[0m, in \u001b[0;36mConditionalRandomField.logprob\u001b[0;34m(self, sentence, corpus)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the *conditional* log-probability log p(tags | words) under the current\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03mmodel parameters.  This behaves differently from the parent class, which returns\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03mlog p(tags, words).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03margument, to help with integerization and check that we're integerizing\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03mcorrectly.\"\"\"\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# Integerize the words and tags of the given sentence, which came from the given corpus.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m#isent = self._integerize_sentence(sentence, corpus)\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m \n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m#basically the same thing as given but doing the integerizing elsewhere\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m isent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_integerize_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m numerator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_pass(isent)  \u001b[38;5;66;03m# Follows gold path if supervised\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# For denominator, remove all tag information\u001b[39;00m\n",
      "File \u001b[0;32m~/jhu/senior_year/nlp/nlp_HW7/nlp_hw7/hw-rnn/code/hmm.py:308\u001b[0m, in \u001b[0;36mHiddenMarkovModel._integerize_sentence\u001b[0;34m(self, sentence, corpus)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Integerize the words and tags of the given sentence, which came from the given corpus.\"\"\"\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus\u001b[38;5;241m.\u001b[39mtagset \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagset \u001b[38;5;129;01mor\u001b[39;00m corpus\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;66;03m# Sentence comes from some other corpus that this HMM was not set up to handle.\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe corpus that this sentence came from uses a different tagset or vocab\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m corpus\u001b[38;5;241m.\u001b[39mintegerize_sentence(sentence)\n",
      "\u001b[0;31mTypeError\u001b[0m: The corpus that this sentence came from uses a different tagset or vocab"
     ]
    }
   ],
   "source": [
    "# Baseline CRF\n",
    "baseline_results = train_and_evaluate(\n",
    "    ConditionalRandomFieldBackprop,\n",
    "    train_corpus,\n",
    "    dev_corpus\n",
    ")\n",
    "\n",
    "# BiRNN-CRF\n",
    "birnn_results = train_and_evaluate(\n",
    "    ConditionalRandomFieldNeural,\n",
    "    train_corpus,\n",
    "    dev_corpus,\n",
    "    rnn_dim=10,\n",
    "    lexicon=lexicon,\n",
    "    corpus=train_corpus\n",
    ")\n",
    "\n",
    "print(\"Baseline CRF Results:\")\n",
    "for k, v in baseline_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nBiRNN-CRF Results:\")\n",
    "for k, v in birnn_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different hyperparameter combinations\n",
    "results = []\n",
    "\n",
    "rnn_dims = [16, 32, 64]\n",
    "learning_rates = [0.01, 0.05, 0.1]\n",
    "batch_sizes = [10, 30, 50]\n",
    "reg_values = [0.0, 0.001, 0.01]\n",
    "\n",
    "for rnn_dim in rnn_dims:\n",
    "    for lr in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            for reg in reg_values:\n",
    "                print(f\"Testing: rnn_dim={rnn_dim}, lr={lr}, batch_size={batch_size}, reg={reg}\")\n",
    "                \n",
    "                metrics = train_and_evaluate(\n",
    "                    ConditionalRandomFieldNeural,\n",
    "                    train_corpus,\n",
    "                    dev_corpus,\n",
    "                    rnn_dim=rnn_dim,\n",
    "                    lexicon=lexicon,\n",
    "                    corpus=train_corpus,\n",
    "                    lr=lr,\n",
    "                    batch_size=batch_size,\n",
    "                    reg=reg\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    'rnn_dim': rnn_dim,\n",
    "                    'lr': lr,\n",
    "                    'batch_size': batch_size,\n",
    "                    'reg': reg,\n",
    "                    **metrics\n",
    "                })\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize impact of hyperparameters\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "# RNN Dimension Impact\n",
    "sns.boxplot(data=results_df, x='rnn_dim', y='dev_cross_entropy', ax=axes[0,0])\n",
    "axes[0,0].set_title('RNN Dimension vs Cross Entropy')\n",
    "\n",
    "# Learning Rate Impact\n",
    "sns.boxplot(data=results_df, x='lr', y='dev_cross_entropy', ax=axes[0,1])\n",
    "axes[0,1].set_title('Learning Rate vs Cross Entropy')\n",
    "\n",
    "# Batch Size Impact\n",
    "sns.boxplot(data=results_df, x='batch_size', y='dev_cross_entropy', ax=axes[1,0])\n",
    "axes[1,0].set_title('Batch Size vs Cross Entropy')\n",
    "\n",
    "# Regularization Impact\n",
    "sns.boxplot(data=results_df, x='reg', y='dev_cross_entropy', ax=axes[1,1])\n",
    "axes[1,1].set_title('Regularization vs Cross Entropy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Speed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training time vs hyperparameters\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "sns.boxplot(data=results_df, x='rnn_dim', y='training_time', ax=axes[0,0])\n",
    "axes[0,0].set_title('RNN Dimension vs Training Time')\n",
    "\n",
    "sns.boxplot(data=results_df, x='lr', y='training_time', ax=axes[0,1])\n",
    "axes[0,1].set_title('Learning Rate vs Training Time')\n",
    "\n",
    "sns.boxplot(data=results_df, x='batch_size', y='training_time', ax=axes[1,0])\n",
    "axes[1,0].set_title('Batch Size vs Training Time')\n",
    "\n",
    "sns.boxplot(data=results_df, x='reg', y='training_time', ax=axes[1,1])\n",
    "axes[1,1].set_title('Regularization vs Training Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training vs. Evaluation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare training vs dev performance\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(results_df['train_cross_entropy'], results_df['dev_cross_entropy'])\n",
    "plt.xlabel('Training Cross Entropy')\n",
    "plt.ylabel('Dev Cross Entropy')\n",
    "plt.title('Cross Entropy: Training vs Dev')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(results_df['train_error_rate'], results_df['dev_error_rate'])\n",
    "plt.xlabel('Training Error Rate')\n",
    "plt.ylabel('Dev Error Rate')\n",
    "plt.title('Error Rate: Training vs Dev')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
